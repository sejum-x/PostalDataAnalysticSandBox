"""
–ê–Ω–∞–ª—ñ–∑ —á–∞—Å—É –æ–±—Ä–æ–±–∫–∏ –ø–æ—Å–∏–ª–æ–∫ –≤ —Ä–æ–∑—Ä—ñ–∑—ñ –ø–µ—Ä—ñ–æ–¥—ñ–≤ (–ó–∞–≤–¥–∞–Ω–Ω—è 3)
–ü—Ä–∞—Ü—é—î –∑ delivery_periodic_raw_data
"""

import pandas as pd
import numpy as np
from datetime import datetime
import json
import os
import sys

sys.path.append('..')
from config.database_config import DatabaseConfig

class ProcessingTimeAnalyzer:
    def __init__(self):
        self.config = DatabaseConfig()
        self.data = None

    def load_data(self, filepath):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Å–∏—Ä—ñ –¥–∞–Ω—ñ –ø–µ—Ä—ñ–æ–¥–∏—á–Ω–∏—Ö –¥–æ—Å—Ç–∞–≤–æ–∫"""
        try:
            print(f"üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É —á–∞—Å—É –æ–±—Ä–æ–±–∫–∏ –∑ {filepath}")
            self.data = pd.read_csv(filepath)
            print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(self.data)} –∑–∞–ø–∏—Å—ñ–≤ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É —á–∞—Å—É –æ–±—Ä–æ–±–∫–∏")
            return True
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö: {e}")
            return False

    def analyze_processing_times_by_periods(self, filepath=None):
        """
        –ó–∞–≤–¥–∞–Ω–Ω—è 3: –ê–Ω–∞–ª—ñ–∑ —á–∞—Å—É –æ–±—Ä–æ–±–∫–∏ –ø–æ—Å–∏–ª–æ–∫ –≤ —Ä–æ–∑—Ä—ñ–∑—ñ –ø–µ—Ä—ñ–æ–¥—ñ–≤
        """
        if filepath and not self.load_data(filepath):
            return {'error': '–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞–Ω—ñ'}

        if self.data is None or self.data.empty:
            return {'error': '–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É'}

        try:
            print("üîÑ –ê–Ω–∞–ª—ñ–∑ —á–∞—Å—É –æ–±—Ä–æ–±–∫–∏ –ø–æ—Å–∏–ª–æ–∫ –ø–æ –ø–µ—Ä—ñ–æ–¥–∞—Ö...")

            # –°—Ç–≤–æ—Ä—é—î–º–æ –∫–æ–ª–æ–Ω–∫—É –ø–µ—Ä—ñ–æ–¥—É
            self.data['period'] = self.data['start_year'].astype(str) + '-' + \
                                 self.data['start_month'].astype(str).str.zfill(2)

            # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ —á–∏—Å–ª–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏
            numeric_columns = ['processing_time_hours', 'deliveries_count', 'parcel_max_size', 'parcel_max_weight']
            for col in numeric_columns:
                if col in self.data.columns:
                    self.data[col] = pd.to_numeric(self.data[col], errors='coerce')
            self.data[numeric_columns] = self.data[numeric_columns].fillna(0)

            # üìä –ê–ù–ê–õ–Ü–ó –ß–ê–°–£ –û–ë–†–û–ë–ö–ò –ü–û –ü–ï–†–Ü–û–î–ê–• –Ü –¢–ò–ü–ê–• –ü–û–°–ò–õ–û–ö
            period_parcel_processing = self.data.groupby([
                'period', 'parcel_type_name', 'parcel_max_size', 'parcel_max_weight'
            ]).agg({
                'processing_time_hours': ['mean', 'median', 'std', 'min', 'max'],
                'deliveries_count': 'sum',
                'department_id': 'nunique',
                'delivery_id': 'count'
            }).round(2)

            # –°–ø–ª—é—â—É—î–º–æ –∫–æ–ª–æ–Ω–∫–∏
            period_parcel_processing.columns = [
                'avg_processing_time', 'median_processing_time', 'std_processing_time',
                'min_processing_time', 'max_processing_time',
                'total_deliveries', 'departments_handling', 'total_records'
            ]

            period_parcel_processing = period_parcel_processing.fillna(0)

            # –Ü–Ω–¥–µ–∫—Å —Å–∫–ª–∞–¥–Ω–æ—Å—Ç—ñ –æ–±—Ä–æ–±–∫–∏ –ø–æ –ø–µ—Ä—ñ–æ–¥–∞—Ö
            period_parcel_processing['period_complexity_score'] = (
                (period_parcel_processing['avg_processing_time'] * 0.5) +
                (period_parcel_processing['std_processing_time'] * 0.3) +
                (period_parcel_processing['max_processing_time'] * 0.2)
            ).round(2)

            # üìà –¢–†–ï–ù–î–ò –ß–ê–°–£ –û–ë–†–û–ë–ö–ò –ü–û –¢–ò–ü–ê–• –ü–û–°–ò–õ–û–ö
            processing_trends = self.data.groupby(['parcel_type_name', 'period']).agg({
                'processing_time_hours': 'mean',
                'deliveries_count': 'sum',
                'department_id': 'nunique'
            }).round(2).fillna(0)

            processing_trends.columns = ['avg_processing_time', 'total_deliveries', 'departments_handling']

            # üìä –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –ü–ï–†–Ü–û–î–Ü–í –ü–û –ß–ê–°–£ –û–ë–†–û–ë–ö–ò
            period_comparison = self.data.groupby('period').agg({
                'processing_time_hours': ['mean', 'median', 'std', 'min', 'max'],
                'deliveries_count': 'sum',
                'parcel_type_id': 'nunique',
                'department_id': 'nunique'
            }).round(2).fillna(0)

            period_comparison.columns = [
                'avg_processing_time', 'median_processing_time', 'std_processing_time',
                'min_processing_time', 'max_processing_time',
                'total_deliveries', 'parcel_types', 'departments'
            ]

            # üèÜ –ù–ê–ô–°–ö–õ–ê–î–ù–Ü–®–Ü –ü–û–°–ò–õ–ö–ò –ü–û –ü–ï–†–Ü–û–î–ê–•
            complex_parcels_by_period = {}
            for period in self.data['period'].unique():
                period_data = period_parcel_processing.loc[
                    period_parcel_processing.index.get_level_values(0) == period
                ]
                if not period_data.empty:
                    top_complex = period_data.nlargest(5, 'period_complexity_score')
                    complex_parcels_by_period[period] = self._convert_multiindex_to_dict(top_complex)

            # üìä –ê–ù–ê–õ–Ü–ó –ï–§–ï–ö–¢–ò–í–ù–û–°–¢–Ü –í–Ü–î–î–Ü–õ–ï–ù–¨ –ü–û –ü–ï–†–Ü–û–î–ê–•
            dept_efficiency_by_period = self.data.groupby(['period', 'department_id', 'department_number']).agg({
                'processing_time_hours': 'mean',
                'deliveries_count': 'sum',
                'parcel_type_id': 'nunique'
            }).round(2).fillna(0)

            dept_efficiency_by_period.columns = [
                'avg_processing_time', 'total_deliveries', 'parcel_types_handled'
            ]

            # –†–∞—Ö—É—î–º–æ –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å (–±–µ–∑–ø–µ—á–Ω–µ –¥—ñ–ª–µ–Ω–Ω—è)
            dept_efficiency_by_period['efficiency_ratio'] = (
                dept_efficiency_by_period['total_deliveries'] /
                (dept_efficiency_by_period['avg_processing_time'] + 0.1)
            ).round(2)

            # üèÜ –ù–ê–ô–ï–§–ï–ö–¢–ò–í–ù–Ü–®–Ü –í–Ü–î–î–Ü–õ–ï–ù–ù–Ø –ü–û –ü–ï–†–Ü–û–î–ê–•
            efficient_departments_by_period = {}
            for period in self.data['period'].unique():
                period_data = dept_efficiency_by_period.loc[
                    dept_efficiency_by_period.index.get_level_values(0) == period
                ]
                if not period_data.empty:
                    top_efficient = period_data.nlargest(5, 'efficiency_ratio')
                    efficient_departments_by_period[period] = self._convert_multiindex_to_dict(top_efficient)

            # üìä –ê–ù–ê–õ–Ü–ó –ü–û –†–ï–ì–Ü–û–ù–ê–• –Ü –ü–ï–†–Ü–û–î–ê–•
            region_processing_by_period = self.data.groupby(['period', 'department_region']).agg({
                'processing_time_hours': ['mean', 'median', 'std'],
                'deliveries_count': 'sum',
                'department_id': 'nunique',
                'parcel_type_id': 'nunique'
            }).round(2).fillna(0)

            region_processing_by_period.columns = [
                'avg_processing_time', 'median_processing_time', 'std_processing_time',
                'total_deliveries', 'departments_count', 'parcel_types_count'
            ]

            # üìà –î–ò–ù–ê–ú–Ü–ö–ê –ó–ú–Ü–ù –ü–û –ü–ï–†–Ü–û–î–ê–•
            period_changes = {}
            periods = sorted(self.data['period'].unique())
            for i in range(1, len(periods)):
                prev_period = periods[i-1]
                curr_period = periods[i]

                prev_data = period_comparison.loc[prev_period] if prev_period in period_comparison.index else None
                curr_data = period_comparison.loc[curr_period] if curr_period in period_comparison.index else None

                if prev_data is not None and curr_data is not None:
                    changes = {
                        'previous_period': prev_period,
                        'current_period': curr_period,
                        'avg_processing_time_change': float(curr_data['avg_processing_time'] - prev_data['avg_processing_time']),
                        'deliveries_change': int(curr_data['total_deliveries'] - prev_data['total_deliveries']),
                        'departments_change': int(curr_data['departments'] - prev_data['departments']),
                        'improvement_percentage': float(
                            ((prev_data['avg_processing_time'] - curr_data['avg_processing_time']) /
                             (prev_data['avg_processing_time'] + 0.1)) * 100
                        )
                    }
                    period_changes[f"{prev_period}_to_{curr_period}"] = changes

            # –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            general_stats = {
                'total_periods': int(self.data['period'].nunique()),
                'total_records': int(len(self.data)),
                'avg_processing_time': float(self.data['processing_time_hours'].mean()),
                'median_processing_time': float(self.data['processing_time_hours'].median()),
                'min_processing_time': float(self.data['processing_time_hours'].min()),
                'max_processing_time': float(self.data['processing_time_hours'].max()),
                'total_parcel_types': int(self.data['parcel_type_name'].nunique()),
                'total_departments': int(self.data['department_id'].nunique()),
                'total_deliveries': int(self.data['deliveries_count'].sum()),
                'total_regions': int(self.data['department_region'].nunique())
            }

            # –ó–±–∏—Ä–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—î—é —Ç–∏–ø—ñ–≤
            results = {
                'analysis_type': 'processing_time_by_periods',
                'general_stats': self._convert_numpy_types(general_stats),
                'period_parcel_processing': self._convert_multiindex_to_dict(period_parcel_processing),
                'processing_trends': self._convert_multiindex_to_dict(processing_trends),
                'period_comparison': self._convert_numpy_types(period_comparison.to_dict('index')),
                'complex_parcels_by_period': self._convert_numpy_types(complex_parcels_by_period),
                'department_efficiency_by_period': self._convert_multiindex_to_dict(dept_efficiency_by_period),
                'efficient_departments_by_period': self._convert_numpy_types(efficient_departments_by_period),
                'region_processing_by_period': self._convert_multiindex_to_dict(region_processing_by_period),
                'period_changes': self._convert_numpy_types(period_changes),
                'analysis_timestamp': datetime.now().isoformat()
            }

            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≤ –æ–∫—Ä–µ–º–∏–π —Ñ–∞–π–ª
            self._save_results(results, 'processing_time_by_periods')

            print("‚úÖ –ê–Ω–∞–ª—ñ–∑ —á–∞—Å—É –æ–±—Ä–æ–±–∫–∏ –ø–æ –ø–µ—Ä—ñ–æ–¥–∞—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
            return results

        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª—ñ–∑—ñ —á–∞—Å—É –æ–±—Ä–æ–±–∫–∏ –ø–æ –ø–µ—Ä—ñ–æ–¥–∞—Ö: {e}")
            return {'error': str(e)}

    def _convert_multiindex_to_dict(self, df):
        """–ö–æ–Ω–≤–µ—Ä—Ç—É—î MultiIndex DataFrame –≤ —Å–ª–æ–≤–Ω–∏–∫"""
        result = {}
        for idx, row in df.iterrows():
            if isinstance(idx, tuple):
                key = "_".join([str(i).replace(' ', '_').replace('/', '_').replace('*', 'x') for i in idx])
            else:
                key = str(idx).replace(' ', '_').replace('/', '_').replace('*', 'x')

            # –î–æ–¥–∞—î–º–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —ñ–Ω–¥–µ–∫—Å–∏
            row_dict = row.to_dict()
            if isinstance(idx, tuple):
                index_names = df.index.names if hasattr(df.index, 'names') else []
                for i, index_name in enumerate(index_names):
                    if index_name and i < len(idx):
                        row_dict[index_name] = idx[i]

            result[key] = self._convert_numpy_types(row_dict)

        return result

    def _convert_numpy_types(self, obj):
        """–ö–æ–Ω–≤–µ—Ä—Ç—É—î numpy —Ç–∏–ø–∏ –≤ Python —Ç–∏–ø–∏ –¥–ª—è JSON —Å–µ—Ä—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó"""
        if isinstance(obj, dict):
            return {str(key): self._convert_numpy_types(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_numpy_types(item) for item in obj]
        elif isinstance(obj, tuple):
            return str(obj)
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif pd.isna(obj):
            return None
        else:
            return obj

    def _save_results(self, results, filename_prefix):
        """–ó–±–µ—Ä—ñ–≥–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∞–Ω–∞–ª—ñ–∑—É –≤ –æ–∫—Ä–µ–º—ñ —Ñ–∞–π–ª–∏"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"{filename_prefix}_{timestamp}.json"
            filepath = os.path.join(self.config.PROCESSED_DATA_PATH, filename)

            # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é —è–∫—â–æ –Ω–µ —ñ—Å–Ω—É—î
            os.makedirs(self.config.PROCESSED_DATA_PATH, exist_ok=True)

            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)

            print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {filename}")
            return filepath

        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤: {e}")
            return None