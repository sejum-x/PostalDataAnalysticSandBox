"""
–ê–Ω–∞–ª—ñ–∑ —á–∞—Å—É –æ–±—Ä–æ–±–∫–∏ –ø–æ—Å–∏–ª–æ–∫ (–ó–∞–≤–¥–∞–Ω–Ω—è 3)
–ü—Ä–∞—Ü—é—î –∑ delivery_periodic_raw_data
"""

import pandas as pd
import numpy as np
from datetime import datetime
import json
import os
import sys

sys.path.append('..')
from config.database_config import DatabaseConfig

class ProcessingTimeAnalyzer:
    def __init__(self):
        self.config = DatabaseConfig()
        self.data = None

    def load_data(self, filepath):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Å–∏—Ä—ñ –¥–∞–Ω—ñ –ø–µ—Ä—ñ–æ–¥–∏—á–Ω–∏—Ö –¥–æ—Å—Ç–∞–≤–æ–∫"""
        try:
            print(f"üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É —á–∞—Å—É –æ–±—Ä–æ–±–∫–∏ –∑ {filepath}")
            self.data = pd.read_csv(filepath)
            print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(self.data)} –∑–∞–ø–∏—Å—ñ–≤ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É —á–∞—Å—É –æ–±—Ä–æ–±–∫–∏")
            return True
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö: {e}")
            return False

    def analyze_processing_times(self, filepath=None):
        """
        –ó–∞–≤–¥–∞–Ω–Ω—è 3: –ê–Ω–∞–ª—ñ–∑ —á–∞—Å—É –æ–±—Ä–æ–±–∫–∏ –ø–æ—Å–∏–ª–æ–∫
        """
        if filepath and not self.load_data(filepath):
            return {'error': '–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞–Ω—ñ'}

        if self.data is None or self.data.empty:
            return {'error': '–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É'}

        try:
            print("üîÑ –ê–Ω–∞–ª—ñ–∑ —á–∞—Å—É –æ–±—Ä–æ–±–∫–∏ –ø–æ—Å–∏–ª–æ–∫...")

            # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ —á–∏—Å–ª–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏
            numeric_columns = ['processing_time_hours', 'deliveries_count', 'parcel_max_size', 'parcel_max_weight']
            for col in numeric_columns:
                if col in self.data.columns:
                    self.data[col] = pd.to_numeric(self.data[col], errors='coerce')

            # –ó–∞–ø–æ–≤–Ω—é—î–º–æ NaN –∑–Ω–∞—á–µ–Ω–Ω—è –Ω—É–ª—è–º–∏
            self.data[numeric_columns] = self.data[numeric_columns].fillna(0)

            # –ê–Ω–∞–ª—ñ–∑ –ø–æ —Ç–∏–ø–∞—Ö –ø–æ—Å–∏–ª–æ–∫
            parcel_processing = self.data.groupby(['parcel_type_name', 'parcel_max_size', 'parcel_max_weight']).agg({
                'processing_time_hours': ['mean', 'median', 'std', 'min', 'max'],
                'deliveries_count': 'sum',
                'delivery_id': 'count',
                'department_id': 'nunique'
            }).round(2)

            # –°–ø–ª—é—â—É—î–º–æ –∫–æ–ª–æ–Ω–∫–∏
            parcel_processing.columns = [
                'avg_processing_time', 'median_processing_time', 'std_processing_time',
                'min_processing_time', 'max_processing_time',
                'total_deliveries', 'total_records', 'departments_handling'
            ]

            # –ó–∞–ø–æ–≤–Ω—é—î–º–æ NaN –ø—ñ—Å–ª—è –∞–≥—Ä–µ–≥–∞—Ü—ñ—ó
            parcel_processing = parcel_processing.fillna(0)

            # –†–µ–π—Ç–∏–Ω–≥ —Å–∫–ª–∞–¥–Ω–æ—Å—Ç—ñ –æ–±—Ä–æ–±–∫–∏
            parcel_processing['complexity_score'] = (
                (parcel_processing['avg_processing_time'] * 0.5) +
                (parcel_processing['std_processing_time'] * 0.3) +
                (parcel_processing['max_processing_time'] * 0.2)
            ).round(2)

            # ‚úÖ –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ tuple —ñ–Ω–¥–µ–∫—Å–∏ —Ç–∏–ø—ñ–≤ –ø–æ—Å–∏–ª–æ–∫
            parcel_processing_dict = {}
            for (parcel_type, max_size, max_weight), row in parcel_processing.iterrows():
                key = f"{parcel_type}_{max_size}_{max_weight}".replace(' ', '_').replace('*', 'x')
                parcel_processing_dict[key] = {
                    'parcel_type_name': parcel_type,
                    'parcel_max_size': max_size,
                    'parcel_max_weight': max_weight,
                    **row.to_dict()
                }

            # –ù–∞–π—Å–∫–ª–∞–¥–Ω—ñ—à—ñ –¥–ª—è –æ–±—Ä–æ–±–∫–∏ —Ç–∏–ø–∏ –ø–æ—Å–∏–ª–æ–∫
            complex_parcels = parcel_processing.nlargest(10, 'complexity_score')
            complex_parcels_dict = {}
            for (parcel_type, max_size, max_weight), row in complex_parcels.iterrows():
                key = f"{parcel_type}_{max_size}_{max_weight}".replace(' ', '_').replace('*', 'x')
                complex_parcels_dict[key] = {
                    'parcel_type_name': parcel_type,
                    'parcel_max_size': max_size,
                    'parcel_max_weight': max_weight,
                    **row.to_dict()
                }

            # –ê–Ω–∞–ª—ñ–∑ –ø–æ –≤—ñ–¥–¥—ñ–ª–µ–Ω–Ω—è–º
            dept_processing = self.data.groupby(['department_id', 'department_number', 'department_type']).agg({
                'processing_time_hours': ['mean', 'median', 'std'],
                'deliveries_count': 'sum',
                'parcel_type_id': 'nunique'
            }).round(2).fillna(0)

            dept_processing.columns = [
                'avg_processing_time', 'median_processing_time', 'std_processing_time',
                'total_deliveries', 'parcel_types_handled'
            ]

            # ‚úÖ –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ tuple —ñ–Ω–¥–µ–∫—Å–∏ –≤—ñ–¥–¥—ñ–ª–µ–Ω—å
            dept_processing_dict = {}
            for (dept_id, dept_number, dept_type), row in dept_processing.iterrows():
                key = f"dept_{dept_id}_{dept_number}"
                dept_processing_dict[key] = {
                    'department_id': dept_id,
                    'department_number': dept_number,
                    'department_type': dept_type,
                    **row.to_dict()
                }

            # –ê–Ω–∞–ª—ñ–∑ –ø–æ —Ä–µ–≥—ñ–æ–Ω–∞—Ö
            region_processing = self.data.groupby('department_region').agg({
                'processing_time_hours': ['mean', 'median', 'std'],
                'deliveries_count': 'sum',
                'department_id': 'nunique',
                'parcel_type_id': 'nunique'
            }).round(2).fillna(0)

            region_processing.columns = [
                'avg_processing_time', 'median_processing_time', 'std_processing_time',
                'total_deliveries', 'departments_count', 'parcel_types_count'
            ]

            # –ê–Ω–∞–ª—ñ–∑ –ø–æ –ø–µ—Ä—ñ–æ–¥–∞—Ö
            period_processing = self.data.groupby(['start_year', 'start_month']).agg({
                'processing_time_hours': ['mean', 'median'],
                'deliveries_count': 'sum',
                'delivery_id': 'count'
            }).round(2).fillna(0)

            period_processing.columns = [
                'avg_processing_time', 'median_processing_time',
                'total_deliveries', 'total_records'
            ]

            # ‚úÖ –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ tuple —ñ–Ω–¥–µ–∫—Å–∏ –ø–µ—Ä—ñ–æ–¥—ñ–≤
            period_processing_dict = {}
            for (year, month), row in period_processing.iterrows():
                key = f"{year}_{month:02d}"
                period_processing_dict[key] = {
                    'year': year,
                    'month': month,
                    **row.to_dict()
                }

            # –ê–Ω–∞–ª—ñ–∑ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ (—á–∞—Å –æ–±—Ä–æ–±–∫–∏ vs –∫—ñ–ª—å–∫—ñ—Å—Ç—å –¥–æ—Å—Ç–∞–≤–æ–∫)
            efficiency_analysis = self.data.groupby('department_id').agg({
                'processing_time_hours': 'mean',
                'deliveries_count': 'sum'
            }).fillna(0)

            # –ë–µ–∑–ø–µ—á–Ω–µ –¥—ñ–ª–µ–Ω–Ω—è –¥–ª—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
            efficiency_analysis['efficiency_ratio'] = (
                efficiency_analysis['deliveries_count'] / (efficiency_analysis['processing_time_hours'] + 0.1)
            ).round(2)

            # –¢–æ–ø –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ –≤—ñ–¥–¥—ñ–ª–µ–Ω–Ω—è
            efficient_departments = efficiency_analysis.nlargest(10, 'efficiency_ratio')

            # –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            general_stats = {
                'total_records': int(len(self.data)),
                'avg_processing_time': float(self.data['processing_time_hours'].mean()),
                'median_processing_time': float(self.data['processing_time_hours'].median()),
                'min_processing_time': float(self.data['processing_time_hours'].min()),
                'max_processing_time': float(self.data['processing_time_hours'].max()),
                'total_parcel_types': int(self.data['parcel_type_name'].nunique()),
                'total_departments': int(self.data['department_id'].nunique()),
                'total_deliveries': int(self.data['deliveries_count'].sum())
            }

            # –ó–±–∏—Ä–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—î—é —Ç–∏–ø—ñ–≤
            results = {
                'general_stats': self._convert_numpy_types(general_stats),
                'parcel_type_processing': self._convert_numpy_types(parcel_processing_dict),
                'complex_parcels': self._convert_numpy_types(complex_parcels_dict),
                'department_processing': self._convert_numpy_types(dept_processing_dict),
                'region_processing': self._convert_numpy_types(region_processing.to_dict('index')),
                'period_processing': self._convert_numpy_types(period_processing_dict),
                'efficiency_analysis': self._convert_numpy_types(efficiency_analysis.to_dict('index')),
                'efficient_departments': self._convert_numpy_types(efficient_departments.to_dict('index')),
                'analysis_timestamp': datetime.now().isoformat()
            }

            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
            self._save_results(results, 'processing_time_analysis')

            print("‚úÖ –ê–Ω–∞–ª—ñ–∑ —á–∞—Å—É –æ–±—Ä–æ–±–∫–∏ –ø–æ—Å–∏–ª–æ–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
            return results

        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª—ñ–∑—ñ —á–∞—Å—É –æ–±—Ä–æ–±–∫–∏: {e}")
            return {'error': str(e)}

    def _convert_numpy_types(self, obj):
        """–ö–æ–Ω–≤–µ—Ä—Ç—É—î numpy —Ç–∏–ø–∏ –≤ Python —Ç–∏–ø–∏ –¥–ª—è JSON —Å–µ—Ä—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó"""
        if isinstance(obj, dict):
            return {str(key): self._convert_numpy_types(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_numpy_types(item) for item in obj]
        elif isinstance(obj, tuple):
            return str(obj)
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif pd.isna(obj):
            return None
        else:
            return obj

    def _save_results(self, results, filename_prefix):
        """–ó–±–µ—Ä—ñ–≥–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∞–Ω–∞–ª—ñ–∑—É"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"{filename_prefix}_{timestamp}.json"
            filepath = os.path.join(self.config.PROCESSED_DATA_PATH, filename)

            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)

            print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {filename}")

        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤: {e}")