"""
–ê–Ω–∞–ª—ñ–∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç—É –≤ —Ä–æ–∑—Ä—ñ–∑—ñ –ø–µ—Ä—ñ–æ–¥—ñ–≤ (–ó–∞–≤–¥–∞–Ω–Ω—è 4)
–ü—Ä–∞—Ü—é—î –∑ delivery_periodic_raw_data
"""

import pandas as pd
import numpy as np
from datetime import datetime
import json
import os
import sys

sys.path.append('..')
from config.database_config import DatabaseConfig

class TransportAnalyzer:
    def __init__(self):
        self.config = DatabaseConfig()
        self.data = None

    def load_data(self, filepath):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Å–∏—Ä—ñ –¥–∞–Ω—ñ –ø–µ—Ä—ñ–æ–¥–∏—á–Ω–∏—Ö –¥–æ—Å—Ç–∞–≤–æ–∫"""
        try:
            print(f"üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç—É –∑ {filepath}")
            self.data = pd.read_csv(filepath)
            print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(self.data)} –∑–∞–ø–∏—Å—ñ–≤ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç—É")
            return True
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö: {e}")
            return False

    def analyze_transport_utilization_by_periods(self, filepath=None):
        """
        –ó–∞–≤–¥–∞–Ω–Ω—è 4: –ê–Ω–∞–ª—ñ–∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç—É –≤ —Ä–æ–∑—Ä—ñ–∑—ñ –ø–µ—Ä—ñ–æ–¥—ñ–≤
        """
        if filepath and not self.load_data(filepath):
            return {'error': '–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞–Ω—ñ'}

        if self.data is None or self.data.empty:
            return {'error': '–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É'}

        try:
            print("üîÑ –ê–Ω–∞–ª—ñ–∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç—É –ø–æ –ø–µ—Ä—ñ–æ–¥–∞—Ö...")

            # –°—Ç–≤–æ—Ä—é—î–º–æ –∫–æ–ª–æ–Ω–∫—É –ø–µ—Ä—ñ–æ–¥—É
            self.data['period'] = self.data['start_year'].astype(str) + '-' + \
                                 self.data['start_month'].astype(str).str.zfill(2)

            # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ —á–∏—Å–ª–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏
            numeric_columns = ['deliveries_count', 'processing_time_hours', 'deliveries_share_percentage',
                              'parcel_max_weight', 'parcel_max_size']
            for col in numeric_columns:
                if col in self.data.columns:
                    self.data[col] = pd.to_numeric(self.data[col], errors='coerce')
            self.data[numeric_columns] = self.data[numeric_columns].fillna(0)

            # üìä –í–ò–ö–û–†–ò–°–¢–ê–ù–ù–Ø –¢–†–ê–ù–°–ü–û–†–¢–£ –ü–û –ü–ï–†–Ü–û–î–ê–•
            period_transport_usage = self.data.groupby([
                'period', 'transport_body_type_id', 'transport_type_name'
            ]).agg({
                'deliveries_count': 'sum',
                'processing_time_hours': 'mean',
                'deliveries_share_percentage': 'mean',
                'department_id': 'nunique',
                'parcel_type_id': 'nunique',
                'delivery_id': 'count'
            }).round(2).fillna(0)

            period_transport_usage.columns = [
                'total_deliveries', 'avg_processing_time', 'avg_share_percentage',
                'departments_served', 'parcel_types_handled', 'total_records'
            ]

            # –†–µ–π—Ç–∏–Ω–≥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ø–æ –ø–µ—Ä—ñ–æ–¥–∞—Ö
            period_transport_usage['period_utilization_score'] = (
                (period_transport_usage['total_deliveries'] * 0.4) +
                (period_transport_usage['departments_served'] * 0.3) +
                (period_transport_usage['avg_share_percentage'] * 0.3)
            ).round(2)

            # üìà –¢–†–ï–ù–î–ò –í–ò–ö–û–†–ò–°–¢–ê–ù–ù–Ø –¢–†–ê–ù–°–ü–û–†–¢–£
            transport_trends = self.data.groupby(['transport_type_name', 'period']).agg({
                'deliveries_count': 'sum',
                'processing_time_hours': 'mean',
                'deliveries_share_percentage': 'mean',
                'department_id': 'nunique'
            }).round(2).fillna(0)

            transport_trends.columns = [
                'total_deliveries', 'avg_processing_time',
                'avg_share_percentage', 'departments_served'
            ]

            # üìä –ï–§–ï–ö–¢–ò–í–ù–Ü–°–¢–¨ –¢–†–ê–ù–°–ü–û–†–¢–£ –ü–û –ü–ï–†–Ü–û–î–ê–•
            transport_efficiency_by_period = self.data.groupby([
                'period', 'transport_type_name'
            ]).agg({
                'deliveries_count': 'sum',
                'processing_time_hours': 'mean',
                'deliveries_share_percentage': 'mean'
            }).round(2).fillna(0)

            transport_efficiency_by_period.columns = [
                'total_deliveries', 'avg_processing_time', 'avg_share_percentage'
            ]

            # –ë–µ–∑–ø–µ—á–Ω–µ –¥—ñ–ª–µ–Ω–Ω—è –¥–ª—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
            transport_efficiency_by_period['efficiency_ratio'] = (
                transport_efficiency_by_period['total_deliveries'] /
                (transport_efficiency_by_period['avg_processing_time'] + 0.1)
            ).round(2)

            # üèÜ –ù–ê–ô–ï–§–ï–ö–¢–ò–í–ù–Ü–®–ò–ô –¢–†–ê–ù–°–ü–û–†–¢ –ü–û –ü–ï–†–Ü–û–î–ê–•
            efficient_transport_by_period = {}
            for period in self.data['period'].unique():
                period_data = transport_efficiency_by_period.loc[
                    transport_efficiency_by_period.index.get_level_values(0) == period
                ]
                if not period_data.empty:
                    top_efficient = period_data.nlargest(3, 'efficiency_ratio')
                    efficient_transport_by_period[period] = self._convert_multiindex_to_dict(top_efficient)

            # üìä –ê–ù–ê–õ–Ü–ó –¢–†–ê–ù–°–ü–û–†–¢–£ –ü–û –¢–ò–ü–ê–• –ü–û–°–ò–õ–û–ö –Ü –ü–ï–†–Ü–û–î–ê–•
            transport_parcel_period_analysis = self.data.groupby([
                'period', 'transport_type_name', 'parcel_type_name'
            ]).agg({
                'deliveries_count': 'sum',
                'processing_time_hours': 'mean',
                'parcel_max_weight': 'mean',
                'parcel_max_size': 'mean',
                'department_id': 'nunique'
            }).round(2).fillna(0)

            transport_parcel_period_analysis.columns = [
                'total_deliveries', 'avg_processing_time',
                'avg_parcel_weight', 'avg_parcel_size', 'departments_served'
            ]

            # üìä –ê–ù–ê–õ–Ü–ó –¢–†–ê–ù–°–ü–û–†–¢–£ –ü–û –†–ï–ì–Ü–û–ù–ê–• –Ü –ü–ï–†–Ü–û–î–ê–•
            transport_region_period_analysis = self.data.groupby([
                'period', 'transport_type_name', 'department_region'
            ]).agg({
                'deliveries_count': 'sum',
                'processing_time_hours': 'mean',
                'department_id': 'nunique',
                'parcel_type_id': 'nunique'
            }).round(2).fillna(0)

            transport_region_period_analysis.columns = [
                'total_deliveries', 'avg_processing_time',
                'departments_count', 'parcel_types_handled'
            ]

            # üìä –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–Ü–°–¢–¨ –¢–†–ê–ù–°–ü–û–†–¢–£ –ü–û –í–Ü–î–î–Ü–õ–ï–ù–ù–Ø–ú –Ü –ü–ï–†–Ü–û–î–ê–•
            dept_transport_period_analysis = self.data.groupby([
                'period', 'department_id', 'department_number'
            ]).agg({
                'transport_body_type_id': 'nunique',
                'deliveries_count': 'sum',
                'processing_time_hours': 'mean',
                'parcel_type_id': 'nunique'
            }).round(2).fillna(0)

            dept_transport_period_analysis.columns = [
                'transport_types_used', 'total_deliveries',
                'avg_processing_time', 'parcel_types_handled'
            ]

            # üìà –î–ò–ù–ê–ú–Ü–ö–ê –ó–ú–Ü–ù –í–ò–ö–û–†–ò–°–¢–ê–ù–ù–Ø –¢–†–ê–ù–°–ü–û–†–¢–£
            transport_changes = {}
            periods = sorted(self.data['period'].unique())

            # –ó–∞–≥–∞–ª—å–Ω—ñ –∑–º—ñ–Ω–∏ –ø–æ –ø–µ—Ä—ñ–æ–¥–∞—Ö
            period_transport_summary = self.data.groupby('period').agg({
                'deliveries_count': 'sum',
                'processing_time_hours': 'mean',
                'transport_body_type_id': 'nunique',
                'department_id': 'nunique'
            }).round(2).fillna(0)

            for i in range(1, len(periods)):
                prev_period = periods[i-1]
                curr_period = periods[i]

                prev_data = period_transport_summary.loc[prev_period] if prev_period in period_transport_summary.index else None
                curr_data = period_transport_summary.loc[curr_period] if curr_period in period_transport_summary.index else None

                if prev_data is not None and curr_data is not None:
                    changes = {
                        'previous_period': prev_period,
                        'current_period': curr_period,
                        'deliveries_change': int(curr_data['deliveries_count'] - prev_data['deliveries_count']),
                        'processing_time_change': float(curr_data['processing_time_hours'] - prev_data['processing_time_hours']),
                        'transport_types_change': int(curr_data['transport_body_type_id'] - prev_data['transport_body_type_id']),
                        'departments_change': int(curr_data['department_id'] - prev_data['department_id'])
                    }
                    transport_changes[f"{prev_period}_to_{curr_period}"] = changes

            # üèÜ –ù–ê–ô–ë–Ü–õ–¨–® –í–ò–ö–û–†–ò–°–¢–û–í–£–í–ê–ù–Ü –¢–ò–ü–ò –¢–†–ê–ù–°–ü–û–†–¢–£ –ü–û –ü–ï–†–Ü–û–î–ê–•
            most_used_transport_by_period = {}
            for period in self.data['period'].unique():
                period_data = period_transport_usage.loc[
                    period_transport_usage.index.get_level_values(0) == period
                ]
                if not period_data.empty:
                    top_used = period_data.nlargest(5, 'period_utilization_score')
                    most_used_transport_by_period[period] = self._convert_multiindex_to_dict(top_used)

            # –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            general_stats = {
                'total_periods': int(self.data['period'].nunique()),
                'total_transport_types': int(self.data['transport_type_name'].nunique()),
                'total_records': int(len(self.data)),
                'total_deliveries': int(self.data['deliveries_count'].sum()),
                'avg_processing_time': float(self.data['processing_time_hours'].mean()),
                'departments_using_transport': int(self.data['department_id'].nunique()),
                'parcel_types_transported': int(self.data['parcel_type_name'].nunique()),
                'regions_served': int(self.data['department_region'].nunique()),
                'avg_share_percentage': float(self.data['deliveries_share_percentage'].mean())
            }

            # –ó–±–∏—Ä–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—î—é —Ç–∏–ø—ñ–≤
            results = {
                'analysis_type': 'transport_utilization_by_periods',
                'general_stats': self._convert_numpy_types(general_stats),
                'period_transport_usage': self._convert_multiindex_to_dict(period_transport_usage),
                'transport_trends': self._convert_multiindex_to_dict(transport_trends),
                'transport_efficiency_by_period': self._convert_multiindex_to_dict(transport_efficiency_by_period),
                'efficient_transport_by_period': self._convert_numpy_types(efficient_transport_by_period),
                'transport_parcel_period_analysis': self._convert_multiindex_to_dict(transport_parcel_period_analysis),
                'transport_region_period_analysis': self._convert_multiindex_to_dict(transport_region_period_analysis),
                'department_transport_period_analysis': self._convert_multiindex_to_dict(dept_transport_period_analysis),
                'most_used_transport_by_period': self._convert_numpy_types(most_used_transport_by_period),
                'transport_changes': self._convert_numpy_types(transport_changes),
                'analysis_timestamp': datetime.now().isoformat()
            }

            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≤ –æ–∫—Ä–µ–º–∏–π —Ñ–∞–π–ª
            self._save_results(results, 'transport_utilization_by_periods')

            print("‚úÖ –ê–Ω–∞–ª—ñ–∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç—É –ø–æ –ø–µ—Ä—ñ–æ–¥–∞—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
            return results

        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª—ñ–∑—ñ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç—É –ø–æ –ø–µ—Ä—ñ–æ–¥–∞—Ö: {e}")
            return {'error': str(e)}

    def _convert_multiindex_to_dict(self, df):
        """–ö–æ–Ω–≤–µ—Ä—Ç—É—î MultiIndex DataFrame –≤ —Å–ª–æ–≤–Ω–∏–∫"""
        result = {}
        for idx, row in df.iterrows():
            if isinstance(idx, tuple):
                key = "_".join([str(i).replace(' ', '_').replace('/', '_').replace('-', '_') for i in idx])
            else:
                key = str(idx).replace(' ', '_').replace('/', '_').replace('-', '_')

            # –î–æ–¥–∞—î–º–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —ñ–Ω–¥–µ–∫—Å–∏
            row_dict = row.to_dict()
            if isinstance(idx, tuple):
                index_names = df.index.names if hasattr(df.index, 'names') else []
                for i, index_name in enumerate(index_names):
                    if index_name and i < len(idx):
                        row_dict[index_name] = idx[i]

            result[key] = self._convert_numpy_types(row_dict)

        return result

    def _convert_numpy_types(self, obj):
        """–ö–æ–Ω–≤–µ—Ä—Ç—É—î numpy —Ç–∏–ø–∏ –≤ Python —Ç–∏–ø–∏ –¥–ª—è JSON —Å–µ—Ä—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó"""
        if isinstance(obj, dict):
            return {str(key): self._convert_numpy_types(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_numpy_types(item) for item in obj]
        elif isinstance(obj, tuple):
            return str(obj)
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif pd.isna(obj):
            return None
        else:
            return obj

    def _save_results(self, results, filename_prefix):
        """–ó–±–µ—Ä—ñ–≥–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∞–Ω–∞–ª—ñ–∑—É –≤ –æ–∫—Ä–µ–º—ñ —Ñ–∞–π–ª–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è—Ö"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

            # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é —è–∫—â–æ –Ω–µ —ñ—Å–Ω—É—î
            os.makedirs(self.config.PROCESSED_DATA_PATH, exist_ok=True)

            saved_files = []

            # 1. –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            general_stats_file = f"transport_general_stats_{timestamp}.json"
            general_stats_path = os.path.join(self.config.PROCESSED_DATA_PATH, general_stats_file)
            with open(general_stats_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'analysis_type': 'transport_general_stats',
                    'data': results['general_stats'],
                    'analysis_timestamp': results['analysis_timestamp']
                }, f, ensure_ascii=False, indent=2)
            saved_files.append(general_stats_file)

            # 2. –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç—É –ø–æ –ø–µ—Ä—ñ–æ–¥–∞—Ö
            period_usage_file = f"transport_period_usage_{timestamp}.json"
            period_usage_path = os.path.join(self.config.PROCESSED_DATA_PATH, period_usage_file)
            with open(period_usage_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'analysis_type': 'transport_period_usage',
                    'data': results['period_transport_usage'],
                    'analysis_timestamp': results['analysis_timestamp']
                }, f, ensure_ascii=False, indent=2)
            saved_files.append(period_usage_file)

            # 3. –¢—Ä–µ–Ω–¥–∏ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç—É
            transport_trends_file = f"transport_trends_{timestamp}.json"
            transport_trends_path = os.path.join(self.config.PROCESSED_DATA_PATH, transport_trends_file)
            with open(transport_trends_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'analysis_type': 'transport_trends',
                    'data': results['transport_trends'],
                    'analysis_timestamp': results['analysis_timestamp']
                }, f, ensure_ascii=False, indent=2)
            saved_files.append(transport_trends_file)

            # 4. –ï—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç—É
            efficiency_file = f"transport_efficiency_{timestamp}.json"
            efficiency_path = os.path.join(self.config.PROCESSED_DATA_PATH, efficiency_file)
            with open(efficiency_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'analysis_type': 'transport_efficiency',
                    'data': results['transport_efficiency_by_period'],
                    'analysis_timestamp': results['analysis_timestamp']
                }, f, ensure_ascii=False, indent=2)
            saved_files.append(efficiency_file)

            # 5. –ï—Ñ–µ–∫—Ç–∏–≤–Ω–∏–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç –ø–æ –ø–µ—Ä—ñ–æ–¥–∞—Ö
            efficient_transport_file = f"transport_efficient_by_period_{timestamp}.json"
            efficient_transport_path = os.path.join(self.config.PROCESSED_DATA_PATH, efficient_transport_file)
            with open(efficient_transport_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'analysis_type': 'transport_efficient_by_period',
                    'data': results['efficient_transport_by_period'],
                    'analysis_timestamp': results['analysis_timestamp']
                }, f, ensure_ascii=False, indent=2)
            saved_files.append(efficient_transport_file)

            # 6. –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç —ñ –ø–æ—Å–∏–ª–∫–∏ –ø–æ –ø–µ—Ä—ñ–æ–¥–∞—Ö
            parcel_analysis_file = f"transport_parcel_analysis_{timestamp}.json"
            parcel_analysis_path = os.path.join(self.config.PROCESSED_DATA_PATH, parcel_analysis_file)
            with open(parcel_analysis_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'analysis_type': 'transport_parcel_analysis',
                    'data': results['transport_parcel_period_analysis'],
                    'analysis_timestamp': results['analysis_timestamp']
                }, f, ensure_ascii=False, indent=2)
            saved_files.append(parcel_analysis_file)

            # 7. –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç –ø–æ —Ä–µ–≥—ñ–æ–Ω–∞—Ö
            region_analysis_file = f"transport_region_analysis_{timestamp}.json"
            region_analysis_path = os.path.join(self.config.PROCESSED_DATA_PATH, region_analysis_file)
            with open(region_analysis_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'analysis_type': 'transport_region_analysis',
                    'data': results['transport_region_period_analysis'],
                    'analysis_timestamp': results['analysis_timestamp']
                }, f, ensure_ascii=False, indent=2)
            saved_files.append(region_analysis_file)

            # 8. –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç –ø–æ –≤—ñ–¥–¥—ñ–ª–µ–Ω–Ω—è–º
            dept_analysis_file = f"transport_department_analysis_{timestamp}.json"
            dept_analysis_path = os.path.join(self.config.PROCESSED_DATA_PATH, dept_analysis_file)
            with open(dept_analysis_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'analysis_type': 'transport_department_analysis',
                    'data': results['department_transport_period_analysis'],
                    'analysis_timestamp': results['analysis_timestamp']
                }, f, ensure_ascii=False, indent=2)
            saved_files.append(dept_analysis_file)

            # 9. –ù–∞–π–±—ñ–ª—å—à –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–Ω–∏–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç
            most_used_file = f"transport_most_used_{timestamp}.json"
            most_used_path = os.path.join(self.config.PROCESSED_DATA_PATH, most_used_file)
            with open(most_used_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'analysis_type': 'transport_most_used',
                    'data': results['most_used_transport_by_period'],
                    'analysis_timestamp': results['analysis_timestamp']
                }, f, ensure_ascii=False, indent=2)
            saved_files.append(most_used_file)

            # 10. –ó–º—ñ–Ω–∏ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç—É
            changes_file = f"transport_changes_{timestamp}.json"
            changes_path = os.path.join(self.config.PROCESSED_DATA_PATH, changes_file)
            with open(changes_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'analysis_type': 'transport_changes',
                    'data': results['transport_changes'],
                    'analysis_timestamp': results['analysis_timestamp']
                }, f, ensure_ascii=False, indent=2)
            saved_files.append(changes_file)

            print(f"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–æ {len(saved_files)} —Ñ–∞–π–ª—ñ–≤: {', '.join(saved_files)}")
            return saved_files

        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤: {e}")
            return None