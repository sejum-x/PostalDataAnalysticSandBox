"""
–ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—å –≤—ñ–¥–¥—ñ–ª–µ–Ω—å –≤ —Ä–æ–∑—Ä—ñ–∑—ñ –ø–µ—Ä—ñ–æ–¥—ñ–≤ (–ó–∞–≤–¥–∞–Ω–Ω—è 2)
–ü—Ä–∞—Ü—é—î –∑ delivery_periodic_raw_data
"""

import pandas as pd
import numpy as np
from datetime import datetime
import json
import os
import sys

sys.path.append('..')
from config.database_config import DatabaseConfig

class DepartmentAnalyzer:
    def __init__(self):
        self.config = DatabaseConfig()
        self.data = None

    def load_data(self, filepath):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Å–∏—Ä—ñ –¥–∞–Ω—ñ –ø–µ—Ä—ñ–æ–¥–∏—á–Ω–∏—Ö –¥–æ—Å—Ç–∞–≤–æ–∫"""
        try:
            print(f"üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –≤—ñ–¥–¥—ñ–ª–µ–Ω—å –∑ {filepath}")
            self.data = pd.read_csv(filepath)
            print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(self.data)} –∑–∞–ø–∏—Å—ñ–≤ –ø–µ—Ä—ñ–æ–¥–∏—á–Ω–∏—Ö –¥–æ—Å—Ç–∞–≤–æ–∫")
            return True
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö: {e}")
            return False

    def analyze_department_workload_by_periods(self, filepath=None):
        """
        –ó–∞–≤–¥–∞–Ω–Ω—è 2: –ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—å –≤—ñ–¥–¥—ñ–ª–µ–Ω—å –≤ —Ä–æ–∑—Ä—ñ–∑—ñ –ø–µ—Ä—ñ–æ–¥—ñ–≤
        """
        if filepath and not self.load_data(filepath):
            return {'error': '–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞–Ω—ñ'}

        if self.data is None or self.data.empty:
            return {'error': '–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É'}

        try:
            print("üîÑ –ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—å –≤—ñ–¥–¥—ñ–ª–µ–Ω—å –ø–æ –ø–µ—Ä—ñ–æ–¥–∞—Ö...")

            # –°—Ç–≤–æ—Ä—é—î–º–æ –∫–æ–ª–æ–Ω–∫—É –ø–µ—Ä—ñ–æ–¥—É
            self.data['period'] = self.data['start_year'].astype(str) + '-' + \
                                 self.data['start_month'].astype(str).str.zfill(2)

            # ‚úÖ –í–ò–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–∞–≤–∏–ª—å–Ω–µ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–∞—Ç –∑ —ñ—Å–Ω—É—é—á–∏–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏
            try:
                # –ó–∞–ø–æ–≤–Ω—é—î–º–æ NaN –∑–Ω–∞—á–µ–Ω–Ω—è
                date_columns = ['start_year', 'start_month', 'start_day', 'end_year', 'end_month', 'end_day']
                for col in date_columns:
                    self.data[col] = pd.to_numeric(self.data[col], errors='coerce').fillna(1)

                # –°—Ç–≤–æ—Ä—é—î–º–æ DataFrame –∑ –¥–∞—Ç–∞–º–∏ –¥–ª—è pd.to_datetime
                start_dates_df = self.data[['start_year', 'start_month', 'start_day']].copy()
                start_dates_df.columns = ['year', 'month', 'day']

                end_dates_df = self.data[['end_year', 'end_month', 'end_day']].copy()
                end_dates_df.columns = ['year', 'month', 'day']

                # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤ –¥–∞—Ç–∏
                start_dates = pd.to_datetime(start_dates_df)
                end_dates = pd.to_datetime(end_dates_df)

                # –†–∞—Ö—É—î–º–æ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –ø–µ—Ä—ñ–æ–¥—É
                self.data['period_duration_days'] = (end_dates - start_dates).dt.days + 1

                print(f"‚úÖ –£—Å–ø—ñ—à–Ω–æ —Å—Ç–≤–æ—Ä–µ–Ω–æ –¥–∞—Ç–∏. –°–µ—Ä–µ–¥–Ω—è —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –ø–µ—Ä—ñ–æ–¥—É: {self.data['period_duration_days'].mean():.1f} –¥–Ω—ñ–≤")

            except Exception as date_error:
                print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–∞—Ç: {date_error}")
                print("–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ñ—ñ–∫—Å–æ–≤–∞–Ω—É —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –ø–µ—Ä—ñ–æ–¥—É")
                self.data['period_duration_days'] = 30

            # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ —á–∏—Å–ª–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏
            numeric_columns = ['deliveries_count', 'processing_time_hours', 'deliveries_share_percentage']
            for col in numeric_columns:
                if col in self.data.columns:
                    self.data[col] = pd.to_numeric(self.data[col], errors='coerce')
            self.data[numeric_columns] = self.data[numeric_columns].fillna(0)

            # üìä –ê–ù–ê–õ–Ü–ó –ü–û –ü–ï–†–Ü–û–î–ê–• –Ü –í–Ü–î–î–Ü–õ–ï–ù–ù–Ø–ú
            period_dept_analysis = self.data.groupby([
                'period', 'department_id', 'department_number', 'department_type'
            ]).agg({
                'deliveries_count': 'sum',
                'processing_time_hours': 'mean',
                'deliveries_share_percentage': 'mean',
                'parcel_type_id': 'nunique',
                'period_duration_days': 'first'
            }).round(2).fillna(0)

            period_dept_analysis.columns = [
                'total_deliveries', 'avg_processing_time', 'avg_share_percentage',
                'parcel_types_handled', 'period_days'
            ]

            # –†–∞—Ö—É—î–º–æ –¥–æ—Å—Ç–∞–≤–∫–∏ –Ω–∞ –¥–µ–Ω—å
            period_dept_analysis['deliveries_per_day'] = (
                period_dept_analysis['total_deliveries'] /
                period_dept_analysis['period_days'].replace(0, 1)
            ).round(2)

            # –Ü–Ω–¥–µ–∫—Å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ—Å—Ç—ñ –ø–æ –ø–µ—Ä—ñ–æ–¥–∞—Ö
            period_dept_analysis['period_workload_score'] = (
                (period_dept_analysis['deliveries_per_day'] * 0.4) +
                (period_dept_analysis['avg_processing_time'] * 0.3) +
                (period_dept_analysis['avg_share_percentage'] * 0.3)
            ).round(2)

            # üìà –¢–†–ï–ù–î–ò –ü–û –í–Ü–î–î–Ü–õ–ï–ù–ù–Ø–ú
            dept_trends = self.data.groupby(['department_id', 'department_number', 'period']).agg({
                'deliveries_count': 'sum',
                'processing_time_hours': 'mean',
                'deliveries_share_percentage': 'mean'
            }).round(2).fillna(0)

            dept_trends.columns = ['total_deliveries', 'avg_processing_time', 'avg_share_percentage']

            # üìä –ó–ê–ì–ê–õ–¨–ù–ê –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ü–ï–†–Ü–û–î–ê–•
            period_summary = self.data.groupby('period').agg({
                'deliveries_count': 'sum',
                'processing_time_hours': 'mean',
                'department_id': 'nunique',
                'parcel_type_id': 'nunique',
                'transport_body_type_id': 'nunique'
            }).round(2).fillna(0)

            period_summary.columns = [
                'total_deliveries', 'avg_processing_time',
                'active_departments', 'parcel_types', 'transport_types'
            ]

            # üèÜ –¢–û–ü –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–Ü –í–Ü–î–î–Ü–õ–ï–ù–ù–Ø –ü–û –ü–ï–†–Ü–û–î–ê–•
            top_busy_by_period = {}
            for period in self.data['period'].unique():
                period_data = period_dept_analysis.loc[
                    period_dept_analysis.index.get_level_values(0) == period
                ]
                if not period_data.empty:
                    top_5 = period_data.nlargest(5, 'period_workload_score')
                    top_busy_by_period[period] = self._convert_multiindex_to_dict(top_5)

            # üìä –ê–ù–ê–õ–Ü–ó –ü–û –¢–ò–ü–ê–• –í–Ü–î–î–Ü–õ–ï–ù–¨ –Ü –ü–ï–†–Ü–û–î–ê–•
            dept_type_period_analysis = self.data.groupby(['period', 'department_type']).agg({
                'deliveries_count': 'sum',
                'processing_time_hours': 'mean',
                'department_id': 'nunique'
            }).round(2).fillna(0)

            dept_type_period_analysis.columns = [
                'total_deliveries', 'avg_processing_time', 'departments_count'
            ]

            # üìä –ê–ù–ê–õ–Ü–ó –ü–û –†–ï–ì–Ü–û–ù–ê–• –Ü –ü–ï–†–Ü–û–î–ê–•
            region_period_analysis = self.data.groupby(['period', 'department_region']).agg({
                'deliveries_count': 'sum',
                'processing_time_hours': 'mean',
                'department_id': 'nunique',
                'parcel_type_id': 'nunique'
            }).round(2).fillna(0)

            region_period_analysis.columns = [
                'total_deliveries', 'avg_processing_time',
                'departments_count', 'parcel_types'
            ]

            # üìà –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –ü–ï–†–Ü–û–î–Ü–í
            period_comparison = {}
            periods = sorted(self.data['period'].unique())
            for i in range(1, len(periods)):
                prev_period = periods[i-1]
                curr_period = periods[i]

                prev_data = period_summary.loc[prev_period] if prev_period in period_summary.index else None
                curr_data = period_summary.loc[curr_period] if curr_period in period_summary.index else None

                if prev_data is not None and curr_data is not None:
                    comparison = {
                        'previous_period': prev_period,
                        'current_period': curr_period,
                        'deliveries_change': float(curr_data['total_deliveries'] - prev_data['total_deliveries']),
                        'processing_time_change': float(curr_data['avg_processing_time'] - prev_data['avg_processing_time']),
                        'departments_change': int(curr_data['active_departments'] - prev_data['active_departments'])
                    }
                    period_comparison[f"{prev_period}_to_{curr_period}"] = comparison

            # üìä –ê–ù–ê–õ–Ü–ó –ü–û –ú–Ü–°–¢–ê–• –Ü –ü–ï–†–Ü–û–î–ê–•
            city_period_analysis = self.data.groupby(['period', 'department_city', 'department_region']).agg({
                'deliveries_count': 'sum',
                'processing_time_hours': 'mean',
                'department_id': 'nunique'
            }).round(2).fillna(0)

            city_period_analysis.columns = [
                'total_deliveries', 'avg_processing_time', 'departments_count'
            ]

            # –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            general_stats = {
                'total_periods': int(self.data['period'].nunique()),
                'total_departments': int(self.data['department_id'].nunique()),
                'total_records': int(len(self.data)),
                'total_deliveries': int(self.data['deliveries_count'].sum()),
                'avg_processing_time': float(self.data['processing_time_hours'].mean()),
                'total_regions': int(self.data['department_region'].nunique()),
                'total_cities': int(self.data['department_city'].nunique()),
                'parcel_types': int(self.data['parcel_type_name'].nunique()),
                'transport_types': int(self.data['transport_type_name'].nunique()),
                'department_types': int(self.data['department_type'].nunique()),
                'avg_period_duration': float(self.data['period_duration_days'].mean())
            }

            # –ó–±–∏—Ä–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—î—é —Ç–∏–ø—ñ–≤
            results = {
                'analysis_type': 'department_workload_by_periods',
                'general_stats': self._convert_numpy_types(general_stats),
                'period_department_analysis': self._convert_multiindex_to_dict(period_dept_analysis),
                'period_summary': self._convert_numpy_types(period_summary.to_dict('index')),
                'department_trends': self._convert_multiindex_to_dict(dept_trends),
                'top_busy_departments_by_period': self._convert_numpy_types(top_busy_by_period),
                'department_type_period_analysis': self._convert_multiindex_to_dict(dept_type_period_analysis),
                'region_period_analysis': self._convert_multiindex_to_dict(region_period_analysis),
                'city_period_analysis': self._convert_multiindex_to_dict(city_period_analysis),
                'period_comparison': self._convert_numpy_types(period_comparison),
                'analysis_timestamp': datetime.now().isoformat()
            }

            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≤ –æ–∫—Ä–µ–º–∏–π —Ñ–∞–π–ª
            self._save_results(results, 'department_workload_by_periods')

            print("‚úÖ –ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—å –≤—ñ–¥–¥—ñ–ª–µ–Ω—å –ø–æ –ø–µ—Ä—ñ–æ–¥–∞—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
            return results

        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª—ñ–∑—ñ –≤—ñ–¥–¥—ñ–ª–µ–Ω—å –ø–æ –ø–µ—Ä—ñ–æ–¥–∞—Ö: {e}")
            import traceback
            traceback.print_exc()
            return {'error': str(e)}

    def _convert_multiindex_to_dict(self, df):
        """–ö–æ–Ω–≤–µ—Ä—Ç—É—î MultiIndex DataFrame –≤ —Å–ª–æ–≤–Ω–∏–∫"""
        result = {}
        for idx, row in df.iterrows():
            if isinstance(idx, tuple):
                key = "_".join([str(i).replace(' ', '_').replace('/', '_').replace('-', '_') for i in idx])
            else:
                key = str(idx).replace(' ', '_').replace('/', '_').replace('-', '_')

            # –î–æ–¥–∞—î–º–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —ñ–Ω–¥–µ–∫—Å–∏
            row_dict = row.to_dict()
            if isinstance(idx, tuple):
                index_names = df.index.names if hasattr(df.index, 'names') else []
                for i, index_name in enumerate(index_names):
                    if index_name and i < len(idx):
                        row_dict[index_name] = idx[i]

            result[key] = self._convert_numpy_types(row_dict)

        return result

    def _convert_numpy_types(self, obj):
        """–ö–æ–Ω–≤–µ—Ä—Ç—É—î numpy —Ç–∏–ø–∏ –≤ Python —Ç–∏–ø–∏ –¥–ª—è JSON —Å–µ—Ä—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó"""
        if isinstance(obj, dict):
            return {str(key): self._convert_numpy_types(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_numpy_types(item) for item in obj]
        elif isinstance(obj, tuple):
            return str(obj)
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif pd.isna(obj):
            return None
        else:
            return obj

    def _save_results(self, results, filename_prefix):
        """–ó–±–µ—Ä—ñ–≥–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∞–Ω–∞–ª—ñ–∑—É –≤ –æ–∫—Ä–µ–º—ñ —Ñ–∞–π–ª–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è—Ö"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

            # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é —è–∫—â–æ –Ω–µ —ñ—Å–Ω—É—î
            os.makedirs(self.config.PROCESSED_DATA_PATH, exist_ok=True)

            saved_files = []

            # 1. –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            general_stats_file = f"department_general_stats_{timestamp}.json"
            general_stats_path = os.path.join(self.config.PROCESSED_DATA_PATH, general_stats_file)
            with open(general_stats_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'analysis_type': 'department_general_stats',
                    'data': results['general_stats'],
                    'analysis_timestamp': results['analysis_timestamp']
                }, f, ensure_ascii=False, indent=2)
            saved_files.append(general_stats_file)

            # 2. –ê–Ω–∞–ª—ñ–∑ –≤—ñ–¥–¥—ñ–ª–µ–Ω—å –ø–æ –ø–µ—Ä—ñ–æ–¥–∞—Ö
            period_dept_file = f"department_period_analysis_{timestamp}.json"
            period_dept_path = os.path.join(self.config.PROCESSED_DATA_PATH, period_dept_file)
            with open(period_dept_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'analysis_type': 'department_period_analysis',
                    'data': results['period_department_analysis'],
                    'analysis_timestamp': results['analysis_timestamp']
                }, f, ensure_ascii=False, indent=2)
            saved_files.append(period_dept_file)

            # 3. –ü—ñ–¥—Å—É–º–∫–∏ –ø–æ –ø–µ—Ä—ñ–æ–¥–∞—Ö
            period_summary_file = f"department_period_summary_{timestamp}.json"
            period_summary_path = os.path.join(self.config.PROCESSED_DATA_PATH, period_summary_file)
            with open(period_summary_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'analysis_type': 'department_period_summary',
                    'data': results['period_summary'],
                    'analysis_timestamp': results['analysis_timestamp']
                }, f, ensure_ascii=False, indent=2)
            saved_files.append(period_summary_file)

            # 4. –¢—Ä–µ–Ω–¥–∏ –≤—ñ–¥–¥—ñ–ª–µ–Ω—å
            dept_trends_file = f"department_trends_{timestamp}.json"
            dept_trends_path = os.path.join(self.config.PROCESSED_DATA_PATH, dept_trends_file)
            with open(dept_trends_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'analysis_type': 'department_trends',
                    'data': results['department_trends'],
                    'analysis_timestamp': results['analysis_timestamp']
                }, f, ensure_ascii=False, indent=2)
            saved_files.append(dept_trends_file)

            # 5. –¢–æ–ø –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—ñ –≤—ñ–¥–¥—ñ–ª–µ–Ω–Ω—è
            top_busy_file = f"department_top_busy_{timestamp}.json"
            top_busy_path = os.path.join(self.config.PROCESSED_DATA_PATH, top_busy_file)
            with open(top_busy_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'analysis_type': 'department_top_busy',
                    'data': results['top_busy_departments_by_period'],
                    'analysis_timestamp': results['analysis_timestamp']
                }, f, ensure_ascii=False, indent=2)
            saved_files.append(top_busy_file)

            # 6. –ê–Ω–∞–ª—ñ–∑ —Ç–∏–ø—ñ–≤ –≤—ñ–¥–¥—ñ–ª–µ–Ω—å
            dept_type_file = f"department_type_analysis_{timestamp}.json"
            dept_type_path = os.path.join(self.config.PROCESSED_DATA_PATH, dept_type_file)
            with open(dept_type_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'analysis_type': 'department_type_analysis',
                    'data': results['department_type_period_analysis'],
                    'analysis_timestamp': results['analysis_timestamp']
                }, f, ensure_ascii=False, indent=2)
            saved_files.append(dept_type_file)

            # 7. –ê–Ω–∞–ª—ñ–∑ –ø–æ —Ä–µ–≥—ñ–æ–Ω–∞—Ö
            region_analysis_file = f"department_region_analysis_{timestamp}.json"
            region_analysis_path = os.path.join(self.config.PROCESSED_DATA_PATH, region_analysis_file)
            with open(region_analysis_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'analysis_type': 'department_region_analysis',
                    'data': results['region_period_analysis'],
                    'analysis_timestamp': results['analysis_timestamp']
                }, f, ensure_ascii=False, indent=2)
            saved_files.append(region_analysis_file)

            # 8. –ê–Ω–∞–ª—ñ–∑ –ø–æ –º—ñ—Å—Ç–∞—Ö
            city_analysis_file = f"department_city_analysis_{timestamp}.json"
            city_analysis_path = os.path.join(self.config.PROCESSED_DATA_PATH, city_analysis_file)
            with open(city_analysis_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'analysis_type': 'department_city_analysis',
                    'data': results['city_period_analysis'],
                    'analysis_timestamp': results['analysis_timestamp']
                }, f, ensure_ascii=False, indent=2)
            saved_files.append(city_analysis_file)

            # 9. –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –ø–µ—Ä—ñ–æ–¥—ñ–≤
            period_comparison_file = f"department_period_comparison_{timestamp}.json"
            period_comparison_path = os.path.join(self.config.PROCESSED_DATA_PATH, period_comparison_file)
            with open(period_comparison_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'analysis_type': 'department_period_comparison',
                    'data': results['period_comparison'],
                    'analysis_timestamp': results['analysis_timestamp']
                }, f, ensure_ascii=False, indent=2)
            saved_files.append(period_comparison_file)

            print(f"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–æ {len(saved_files)} —Ñ–∞–π–ª—ñ–≤: {', '.join(saved_files)}")
            return saved_files

        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤: {e}")
            return None