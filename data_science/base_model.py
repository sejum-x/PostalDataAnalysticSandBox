# data_science/base_model.py
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
import joblib
import os
from datetime import datetime, timedelta
import json
import sys

sys.path.append('..')
from config.database_config import DatabaseConfig


class BaseMLModel:
    """–ë–∞–∑–æ–≤–∏–π –∫–ª–∞—Å –¥–ª—è –≤—Å—ñ—Ö ML –º–æ–¥–µ–ª–µ–π"""

    def __init__(self, model_name):
        self.model_name = model_name
        self.model = None
        self.scaler = StandardScaler()
        self.label_encoders = {}
        self.feature_names = []
        self.target_name = ""
        self.config = DatabaseConfig()
        self.model_path = os.path.join(self.config.PROCESSED_DATA_PATH, 'models')
        os.makedirs(self.model_path, exist_ok=True)

    def prepare_data(self, data, target_column, feature_columns=None):
        """–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è"""
        print(f"üìä –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è –º–æ–¥–µ–ª—ñ {self.model_name}...")

        if feature_columns is None:
            feature_columns = [col for col in data.columns if col != target_column]

        # –í–∏–¥–∞–ª—è—î–º–æ —Ä—è–¥–∫–∏ –∑ –ø—Ä–æ–ø—É—â–µ–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ –≤ —Ü—ñ–ª—å–æ–≤—ñ–π –∑–º—ñ–Ω–Ω—ñ–π
        data_clean = data.dropna(subset=[target_column])

        X = data_clean[feature_columns].copy()
        y = data_clean[target_column].copy()

        # –û–±—Ä–æ–±–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω–∏—Ö –∑–º—ñ–Ω–Ω–∏—Ö
        for column in X.columns:
            if X[column].dtype == 'object':
                if column not in self.label_encoders:
                    self.label_encoders[column] = LabelEncoder()
                    X[column] = self.label_encoders[column].fit_transform(X[column].astype(str))
                else:
                    X[column] = self.label_encoders[column].transform(X[column].astype(str))

        # –ó–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å
        X = X.fillna(X.mean())

        self.feature_names = feature_columns
        self.target_name = target_column

        print(f"‚úÖ –ü—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(X)} –∑–∞–ø–∏—Å—ñ–≤ –∑ {len(feature_columns)} –æ–∑–Ω–∞–∫–∞–º–∏")
        return X, y

    def train_model(self, X, y):
        """–ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ"""
        print(f"üéØ –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ {self.model_name}...")

        # –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω—É —Ç–∞ —Ç–µ—Å—Ç–æ–≤—É –≤–∏–±—ñ—Ä–∫–∏
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        # –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)

        # –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ
        self.model.fit(X_train_scaled, y_train)

        # –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ñ–π –≤–∏–±—ñ—Ä—Ü—ñ
        y_pred = self.model.predict(X_test_scaled)

        # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –º–µ—Ç—Ä–∏–∫
        metrics = {
            'train_r2': self.model.score(X_train_scaled, y_train),
            'test_r2': r2_score(y_test, y_pred),
            'mae': mean_absolute_error(y_test, y_pred),
            'mse': mean_squared_error(y_test, y_pred),
            'rmse': np.sqrt(mean_squared_error(y_test, y_pred))
        }

        print(f"‚úÖ –ú–æ–¥–µ–ª—å –Ω–∞–≤—á–µ–Ω–∞! R¬≤ = {metrics['test_r2']:.3f}")
        return metrics

    def predict(self, X):
        """–ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è"""
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–≤—á–µ–Ω–∞! –°–ø–æ—á–∞—Ç–∫—É –≤–∏–∫–ª–∏—á—Ç–µ train_model()")

        X_scaled = self.scaler.transform(X)
        return self.model.predict(X_scaled)

    def save_model(self):
        """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_filename = f"{self.model_name}_{timestamp}.joblib"
        model_filepath = os.path.join(self.model_path, model_filename)

        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'label_encoders': self.label_encoders,
            'feature_names': self.feature_names,
            'target_name': self.target_name
        }

        joblib.dump(model_data, model_filepath)
        print(f"üíæ –ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–∞: {model_filename}")
        return model_filepath

    def load_model(self, model_filepath):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ"""
        model_data = joblib.load(model_filepath)
        self.model = model_data['model']
        self.scaler = model_data['scaler']
        self.label_encoders = model_data['label_encoders']
        self.feature_names = model_data['feature_names']
        self.target_name = model_data['target_name']
        print(f"üì• –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞: {os.path.basename(model_filepath)}")

    def save_predictions(self, predictions, filename_suffix=""):
        """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø—Ä–æ–≥–Ω–æ–∑—ñ–≤ —É —Ñ–∞–π–ª"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"{self.model_name}_predictions_{filename_suffix}_{timestamp}.json"
        filepath = os.path.join(self.config.PROCESSED_DATA_PATH, filename)

        prediction_data = {
            'model_name': self.model_name,
            'timestamp': timestamp,
            'predictions': predictions
        }

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(prediction_data, f, ensure_ascii=False, indent=2)

        print(f"üíæ –ü—Ä–æ–≥–Ω–æ–∑–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {filename}")
        return filepath